\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zhang2015efficient}
\citation{ozdenizci2016resting}
\citation{Doppelmayr2002289}
\citation{jaeger2001echo}
\citation{maass2002real}
\citation{lipton2015critical}
\citation{bengio1994learning}
\citation{buteneers2008real}
\citation{naderi2010analysis}
\citation{petrosian2001recurrent}
\citation{ubeyli2008multiclass}
\citation{jaeger2001echo}
\citation{lukovsevivcius2012practical}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\citation{jaeger2001echo}
\citation{jaeger2001echo}
\citation{davidverstraeten2009}
\citation{maass2007computational}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical Frameworks}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Echo State Networks (ESNs)}{3}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A basic ESNs architecture taken from \cite  {jaeger2001echo}. The network consists of three layers, an input layer of size K, an internal reservoir of size N and an output layer of size L.}}{3}{figure.1}}
\@writefile{toc}{\contentsline {paragraph}{Propagation steps}{3}{section*.4}}
\citation{lukovsevivcius2012practical}
\citation{jaeger2001echo}
\@writefile{toc}{\contentsline {paragraph}{Training steps}{4}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Prediction steps}{4}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Networks parameters}{4}{section*.7}}
\citation{jaeger2001short}
\citation{schrauwen2007introduction}
\citation{niedermeyer2005electroencephalography}
\citation{cohen2014analyzing}
\citation{cohen2014analyzing}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Resting State Electroencephalogram (EEG)}{5}{subsection.2.2}}
\citation{cohen2011s}
\citation{cohen2014analyzing}
\citation{cohen2014analyzing}
\citation{cohen2014analyzing}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Caption for LOF}}{6}{figure.2}}
\@writefile{toc}{\contentsline {paragraph}{Properties of EEG features for analytics}{6}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Preprocessing}{6}{section*.9}}
\citation{cohen2014analyzing}
\citation{krizhevsky2012imagenet}
\citation{farabet2013learning}
\citation{bordes2014question}
\citation{luong2014addressing}
\citation{comon1994independent}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This plot shows the interconnected relationship between signal and noise in EEG (taken from \cite  {cohen2014analyzing}). The x-axis is the degree of data cleaning that we conduct and the y-axis is the leftover for signal and noise after the cleaning. We can see the distribution of signal and noise with respect to the level of preprocessing. Area left to a implies there is little noise left, area between a and b has a mixture of both noise and signal and the area right to c has mostly noise.}}{7}{figure.3}}
\@writefile{toc}{\contentsline {paragraph}{Artifacts}{7}{section*.10}}
\citation{jung2000removing}
\citation{Winkler2011}
\citation{lotte2007review}
\citation{bennett2000support}
\citation{anderson1996classification}
\citation{palaniappan2005brain}
\citation{balakrishnan2005multilayer}
\citation{millan2004noninvasive}
\citation{millan2000local}
\citation{millan2000local}
\citation{buteneers2008real}
\@writefile{toc}{\contentsline {paragraph}{Classification overview}{8}{section*.11}}
\citation{mitul2013classification}
\citation{ongenae2013time}
\citation{thatcher2005eeg}
\citation{davidson2003affective}
\citation{kounios2008origins}
\citation{wu2014resting}
\citation{subject2016}
\citation{subject2016}
\@writefile{toc}{\contentsline {section}{\numberline {3}Motivation}{9}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{9}{section.4}}
\@writefile{toc}{\contentsline {paragraph}{Data source}{9}{section*.12}}
\citation{jasper1958ten}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The top two images show how the force modulation task is performed, and the bottom image shows what a subject sees during the experiment (taken from \cite  {subject2016}).}}{10}{figure.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A clip of raw data for 8 channels across five seconds}}{11}{figure.5}}
\@writefile{toc}{\contentsline {paragraph}{Preprocessing}{11}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Data processing pipeline}}{12}{figure.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Artifact markers for a sample data stream by MARA are shown. MARA operates on the independent components. For each component, we have three things: a scalp map projection of this particular component (more red means more activity and more blue means lower activity), a power spectrum with artifact probability (the x axis is frequency in Hz, and the y axis is the magnitude in dB) and the if artifact label. }}{12}{figure.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Data stream with corrupted artifacts. The artifact region lies between the two red lines.}}{13}{figure.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Data stream with artifacts removed using ICA and MARA. The artifacts removed region lies between the two red lines.}}{13}{figure.9}}
\citation{klem1999ten}
\citation{delorme2004eeglab}
\@writefile{toc}{\contentsline {paragraph}{Model Building}{14}{section*.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Locations of 12 selected electrode channels.}}{15}{figure.10}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Subject counts in different group after encoding.}}{15}{table.1}}
\citation{lukovsevivcius2012practical}
\citation{kohavi1995study}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The scatter plot of age vs the relative motor training improvement is shown. The y-axis is for age and the x-axis is for the relative motor training improvement. The smaller the x value is, the more one has improved after motor training. Yellow dots are OH; blue dots are OL; purple dots are YH.}}{16}{figure.11}}
\@writefile{toc}{\contentsline {paragraph}{Post-processing}{16}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{Stratified three-fold cross-validation}{16}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Parameter selection}{17}{section*.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Training and testing error with respect to regularization term. The regularization term starts from 0.0000001 to 0.01 (0.0000001 being 1 and 0.01 being 9). Each value to the right on the x-axis, the regularization will be multiplied by a factor of 5.}}{17}{figure.12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{17}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Network dynamics}{17}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The left chart is a sample network output that has been correctly classified, and the right chart is a sample network output that has been wrongly classified}}{18}{figure.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Model performance}{18}{subsection.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Training and testing errors for different models.}}{18}{table.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Sample input dynamics for a correctly classified example.}}{19}{figure.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Sample network dynamics for a correctly classified example.}}{19}{figure.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Confusion Matrix for model a and b. For model a, class 1 is YH, and class 2 is OH. For model b, class 1 is YH, and class 2 is OL. The green texts are marginal correctness percentages and the red ones are the error rates. The top left 2 by 2 matrix shows the raw count and classification results and their corresponding percentages within the overall population. The gray column on the right shows the accuracy of each predicated class, while the row at the bottom shows the accuracy of each true class. The cell in the bottom right shows the overall accuracy. }}{20}{figure.16}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{20}{section.6}}
\@writefile{toc}{\contentsline {paragraph}{Engineering analysis}{20}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Neuroscience analysis}{21}{section*.19}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Classification function between OH and OL}{21}{lstlisting.1}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and future work}{22}{section.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces A mini example for classified input signals in model b. Red means class 1 and blue means class 2. }}{22}{figure.17}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgments}{23}{section.8}}
\bibstyle{unsrt}
\bibdata{thesis}
\bibcite{zhang2015efficient}{1}
\bibcite{ozdenizci2016resting}{2}
\bibcite{Doppelmayr2002289}{3}
\bibcite{jaeger2001echo}{4}
\bibcite{maass2002real}{5}
\bibcite{lipton2015critical}{6}
\bibcite{bengio1994learning}{7}
\bibcite{buteneers2008real}{8}
\bibcite{naderi2010analysis}{9}
\bibcite{petrosian2001recurrent}{10}
\bibcite{ubeyli2008multiclass}{11}
\bibcite{lukovsevivcius2012practical}{12}
\bibcite{davidverstraeten2009}{13}
\bibcite{maass2007computational}{14}
\bibcite{jaeger2001short}{15}
\bibcite{schrauwen2007introduction}{16}
\bibcite{niedermeyer2005electroencephalography}{17}
\bibcite{cohen2014analyzing}{18}
\bibcite{cohen2011s}{19}
\bibcite{krizhevsky2012imagenet}{20}
\bibcite{farabet2013learning}{21}
\bibcite{bordes2014question}{22}
\bibcite{luong2014addressing}{23}
\bibcite{comon1994independent}{24}
\bibcite{jung2000removing}{25}
\bibcite{Winkler2011}{26}
\bibcite{lotte2007review}{27}
\bibcite{bennett2000support}{28}
\bibcite{anderson1996classification}{29}
\bibcite{palaniappan2005brain}{30}
\bibcite{balakrishnan2005multilayer}{31}
\bibcite{millan2004noninvasive}{32}
\bibcite{millan2000local}{33}
\bibcite{mitul2013classification}{34}
\bibcite{ongenae2013time}{35}
\bibcite{thatcher2005eeg}{36}
\bibcite{davidson2003affective}{37}
\bibcite{kounios2008origins}{38}
\bibcite{wu2014resting}{39}
\bibcite{subject2016}{40}
\bibcite{jasper1958ten}{41}
\bibcite{klem1999ten}{42}
\bibcite{delorme2004eeglab}{43}
\bibcite{kohavi1995study}{44}
